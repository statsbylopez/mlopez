<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Statsbylopez on Statsbylopez</title>
    <link>/</link>
    <description>Recent content in Statsbylopez on Statsbylopez</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Wed, 20 Apr 2016 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Rethinking draft curves</title>
      <link>/post/rethinking-draft-curve/</link>
      <pubDate>Wed, 25 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/rethinking-draft-curve/</guid>
      <description>&lt;p&gt;When the Jets traded pick No. 6, No. 37, No. 49, and a 2019 pick to the Colts for pick No. 3, I broke out my trusty &lt;a href=&#34;https://statsbylopez.com/2016/06/22/the-making-and-comparison-of-draft-curves/&#34;&gt;draft curve&lt;/a&gt; to see what it said about the trade.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;Pick Number&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Value&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Team&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;52.5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;to the Jets&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;50.6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;to the Colts&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;37&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;33.5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;to the Colts&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;49&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;28.3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;to the Colts&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Even when you ignore the 2019 pick conveyed to the Colts, the Jets are enormous losers. To be more specific, the Colts aquired 112.4 points worth of draft value in exchange for 52.5 points. Or, perhaps a better way of putting this deal: had the Jets also received back the first pick in the deal, they would’ve just about broken even.&lt;/p&gt;
&lt;p&gt;These findings accord with just about every take I saw on the trade that came from analytically-minded sources. Even on Jimmy Johnson’s outdated version of the trade chart, &lt;a href=&#34;https://ftw.usatoday.com/2018/03/nfl-jets-colts-draft-trade-richard-thaler-nobel-prize&#34;&gt;Indianapolis&lt;/a&gt; comes out ahead.&lt;/p&gt;
&lt;p&gt;But still, something didn’t pass the smell test, and I figured it was worth a go at revisiting my draft curve.&lt;/p&gt;
&lt;div id=&#34;what-if-a-team-wants-a-superstar&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What if a team wants a superstar?&lt;/h2&gt;
&lt;p&gt;One limitation of the draft curve above is that it focuses on the  player performance. For the Jets, pick. No. 3 is worth an  value of 52.5 points over the course of a career (for those scoring at home, this is reflects a player’s career &lt;a href=&#34;https://www.pro-football-reference.com/blog/index37a8.html&#34;&gt;Approximate Value&lt;/a&gt;). But a team may not be drafting for average player performance, and instead, may prefer a shot at a superstar. How can we tweak our draft curve to account for this team preference?&lt;/p&gt;
&lt;p&gt;Instead of focusing on a smoothed average at each line, we can instead set a cutoff for what it means to be a superstar, and adjust our valuation of each pick to estimate the likelihood of landing a player above that cutoff. There are several reasons to  do this, and I generally dislike setting arbitrary &lt;a href=&#34;http://statsbylopez.netlify.com/post/on-the-risks-of-categorizing-a-continuous-variable/&#34;&gt;cutoffs&lt;/a&gt;. However, there are two benefits of thinking this way. First, the valuations of each pick – the likelihood of landing a superstar – are easy to interpret, which may help in the communication of results. Second, only 11 players are allowed on the field at a given time, and so even if you nail a bunch of 7th round picks, you can’t play them all at once.&lt;/p&gt;
&lt;p&gt;There are several ways to identify a super-star. For ease of implementation, I identified the players in the top &lt;code&gt;21/22&lt;/code&gt; percentile, which is a very rough estimation of whether or not that player could have been identified as the best player on the field at any given time point. Using career AV as an outcome, this corresponds with players with a career AV greater than 66. Here’s a sample of players just above and just below the cutoff.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;Name&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Pos&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;CarAV.projected&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Star&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Reggie Nelson&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;DB&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;63&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;FALSE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Thomas Jones&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;RB&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;62&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;FALSE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Adrian Wilson&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;DB&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;64&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;FALSE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Brian Westbrook&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;RB&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;69&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;TRUE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Lawrence Timmons&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;LB&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;72&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;TRUE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Tra Thomas&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;T&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;69&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;TRUE&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Among the players meeting our star criterion listed above, Westbrook, Timmons, and Thomas all made the Pro Bowl at least once in their careers, and each player was also named to at least one all-Pro team, so this seems like a reasonable cutoff to make.&lt;/p&gt;
&lt;p&gt;Here’s a side-by-side comparison of the two competing charts. On the left is my original draft curve, and on the right is the superstar-specific one.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &amp;#39;loess&amp;#39; and formula &amp;#39;y ~ x&amp;#39;
## `geom_smooth()` using method = &amp;#39;loess&amp;#39; and formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-25-building-a-better-draft-curve_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There are notable differences between the two curves. Primarily, the &lt;code&gt;superstar&lt;/code&gt; curve on the right features a much steeper cutoff than the &lt;code&gt;average&lt;/code&gt; curve. The &lt;code&gt;superstar&lt;/code&gt; curve also makes it quite clear that by pick 100, it is exceedingly unlikely to land a superstar player.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;revising-the-jets-trade&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Revising the Jets trade&lt;/h2&gt;
&lt;p&gt;Here’s a look at the Jets/Colts trade using the superstar-based curve.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;Pick Number&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Value&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Team&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.32&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;to the Jets&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.29&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;to the Colts&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;37&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.09&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;to the Colts&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;49&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.07&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;to the Colts&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Although it’s not exactly quite right to simply add these percentages, the Jets’ return of pick No. 3 (32% chance of a star) looks a bit more reasonable. Using the star rates as pick values, the Jets gained 32 points worth of value in exchange for 43 points. That’s the equivalent of losing out on about the 27th pick in the draft (which is much more reasonable than losing out on the 1st pick in the draft).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;building-the-best-curve&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Building the best curve&lt;/h2&gt;
&lt;p&gt;Perhaps a better draft curve incorporates both overall average pick performance  star likelihood. To do that, I scaled the curves above so that they each had the same total draft valuation, took an average, and then re-scaled that average so that the first pick was worth a value of 1.&lt;/p&gt;
&lt;p&gt;Here’s that curve. The curve in the middle (shown in purple) reflects the average between the &lt;code&gt;average&lt;/code&gt; and &lt;code&gt;superstar&lt;/code&gt; curves.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &amp;#39;loess&amp;#39; and formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-25-building-a-better-draft-curve_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Using the so-called &lt;code&gt;blended&lt;/code&gt; draft curve, here’s a final look at the Jets/Colts trade.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;Pick Number&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Value&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Team&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;27.8&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;to the Jets&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;25.4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;to the Colts&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;37&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;10.8&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;to the Colts&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;49&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;8.7&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;to the Colts&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The Colts still made out like bandits – roughly getting about a 60% rate or return on their No. 3 pick – but it doesn’t look as egregious as it initially did. Roughly, the Colts got value worth about the 20th pick in the draft for free by this blended approach.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;final-thoughts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Final thoughts&lt;/h2&gt;
&lt;p&gt;A blended draft curve is by no means final, but it’s one approach for considering how teams aren’t just drafting for average value, and that they are often looking for a superstar. I should also note that this isn’t my idea at all – Michael Schuckers wrote about it in JQAS, a paper you can read &lt;a href=&#34;http://statsportsconsulting.com/main/wp-content/uploads/Schuckers_JQAS_NFL_Draft.pdf&#34;&gt;here&lt;/a&gt;. Schuckers uses a blend of outcomes to make a final draft curve, and there’s no reason to suspect that mine is any better than his.&lt;/p&gt;
&lt;p&gt;Teams  be doing deeper research into making similar curves for themselves. These  include the salary for each pick, our could use better cutoffs to reflect what they are looking for in players (superstar, consistent starter, etc). But for now, hopefully it makes a bit of sense why an initial, smoothed draft curve may be limited.&lt;/p&gt;
&lt;p&gt;If you want to evaluate trades yourself using the blended curve, I uploaded a table &lt;a href=&#34;https://docs.google.com/spreadsheets/d/15_doA989vn-JoWYqktRU3yLH7ciHB4Jd8RS1m2erGZo/edit?usp=sharing&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The Vegas flu looks real — but somehow the Chicago Blackhawks also got sick</title>
      <link>/post/the-vegas-flu-looks-real/</link>
      <pubDate>Wed, 11 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/the-vegas-flu-looks-real/</guid>
      <description>&lt;p&gt;In yesterday’s post, I walked through the use of a &lt;a href=&#34;http://statsbylopez.netlify.com/post/a-state-space-model-to-evaluate-sports-teams/&#34;&gt;state-space&lt;/a&gt; model to evaluate NHL team strength over the course of an NHL season.&lt;/p&gt;
&lt;p&gt;But analyzing team strength is just the start of how we can use this type of framework to analyze betting market data in sports. In today’s post, I’ll ask a different question – What team had the best home advantage? And did perceptions of any teams (cough cough, Vegas) change over the course of the season?&lt;/p&gt;
&lt;div id=&#34;the-vegas-flu&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Vegas Flu&lt;/h2&gt;
&lt;p&gt;Around mid-December – according to a Google search, &lt;a href=&#34;http://www.espn.com/nhl/story/_/id/21825994/nhl-vegas-flu-real-golden-knights-stunning-home-success&#34;&gt;December 20th&lt;/a&gt; – the public became aware of the Golden Knights’ incredible start at home. Vegas started the season 14-2-1 at T-Mobile Arena, shocking pre-season &lt;a href=&#34;http://www.espn.com/nhl/story/_/id/20841111/2017-18-nhl-season-preview-vegas-golden-knights&#34;&gt;predictions&lt;/a&gt; that had the Golden Knights as one of the worst teams in hockey.&lt;/p&gt;
&lt;p&gt;Yesterday’s post used a time-varying model of team strength that, while able to pick up on changes in team strength with Vegas, also assumed that (i) each NHL team had the same home advantage and (ii) there was no impact of team rest on market numbers. However, both of these assumptions are testable with by adding a few more variables to our model.&lt;/p&gt;
&lt;p&gt;Here was our initial model: &lt;span class=&#34;math display&#34;&gt;\[
\text{E}[\text{logit}(p_{i,j,w})] = \theta_{i,w} - \theta_{j,w} + \alpha,
\]&lt;/span&gt; We expand that model to &lt;span class=&#34;math display&#34;&gt;\[
\text{E}[\text{logit}(p_{i,j,w})] = \theta_{i,w} - \theta_{j,w} + \alpha_i,
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which allows for each team &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; to have its own intercept. But we’re not done yet. The final model,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{E}[\text{logit}(p_{i,j,w})] = \theta_{i,w} - \theta_{j,w} + \alpha_i + \beta_1*Rest_i + \beta_2*Rest_j,
\]&lt;/span&gt; includes binary indicators &lt;span class=&#34;math inline&#34;&gt;\(Rest_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Rest_j\)&lt;/span&gt; which indicate whether or not the home (&lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;) or away (&lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;) team was better rested prior to the game. Note that for rest variables, if both teams had the same rest, or both teams had at least one day of rest, these indicators were set to 0. Rest variables are important, as we hope to be able to tease out the benefit of playing at home from the benefit of playing a non-rested opponent at home.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;are-home-advantages-different&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Are home advantages different?&lt;/h2&gt;
&lt;p&gt;Yes, it appears they are.&lt;/p&gt;
&lt;p&gt;Here’s a plot of the posterior distribution of &lt;span class=&#34;math inline&#34;&gt;\(\alpha_i\)&lt;/span&gt; for each team &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/vegas1.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Each teams’ curve corresponds to their estimated prediction of beating an equal-caliber opponent at home. League-wide, the average is about 55%, but some teams lie above and below that average.&lt;/p&gt;
&lt;p&gt;Overall, betting markets linked Vegas with the best home advantage during the 2017/18 season, as judged by the posterior distribution of the Golden Knights’ &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;. However, we can be more precise: there’s about a 90% chance (using the overlap in the density curves) that Vegas was given a better home adjustment than the second best team (LA Kings). Altogether, the gaps between teams with the best and worst home advantages appear unlikely to be due to chance, which matches our findings in our original paper (&lt;a href=&#34;https://arxiv.org/abs/1701.05976&#34;&gt;link&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;did-home-advantages-change&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Did home advantages change?&lt;/h2&gt;
&lt;p&gt;I replicated the same model fit above using both games before and after December 20th. Why that date? First, it’s the date of the ESPN article that publicized the idea of the Vegas Flu. Second, it’s close enough to the halfway mark of the season that it’s likely we have enough home games to judge each team with.&lt;/p&gt;
&lt;p&gt;Here’s the Golden Knights’ home advantage, both before (dark curve) and after (light curve) December 20th.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/vegas2.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Altogether, it looked like the Knights’ expected win percentage at home was given about a 1.75 percentage point bump after December 20th. This appears significant – but there’s also enough overlap in the two density curves that the increase could be due to chance. From a practical standpoint, this increase would plausibly effect the outcome of no more a game or two – but from a betting market standpoint, an increase of 1.75 percentage points is notable.&lt;/p&gt;
&lt;p&gt;Of course, it’d be more precise to consider the Golden Knights’ increase in light of changes to the home advantage intercepts among other teams.&lt;/p&gt;
&lt;p&gt;Here’s a plot with &lt;em&gt;every&lt;/em&gt; teams’ density curves, showing the home advantage both before (in dark) and after (in color) December 20th. Curves shaded in red reflect teams where the home advantage decreased – teams in blue correspond with an increase. I also organized teams in a weird order – from those with the biggest drop (Chicago) to the biggest increase (Vegas).&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/vegas3.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;I feel a bit like Jack Buck here: I can’t believe what I just saw.&lt;/p&gt;
&lt;p&gt;The team with the biggest change in home advantage is not Vegas but Chicago, which observes an estimated win percentage drop by nearly 2 percentage points. Edmonton and Montreal, the two franchises that, combined with the Blackhawks, had the three most disappointmenting seasons in the NHL, also observe drops in perceived home advantage.&lt;/p&gt;
&lt;p&gt;What does this suggest?&lt;/p&gt;
&lt;p&gt;There are likely two possibilities.&lt;/p&gt;
&lt;p&gt;First, market-perceived home advantage correlates to team strength. If the Blackhawks or Oilers stink, it is reasonable to think that playing in Chicago or Edmonton is less of worry for opponents. Fewer fans are showing up, referees are less likely to give the home team a call, and the home team is, even after accounting for team strength, a hair less likely to win.&lt;/p&gt;
&lt;p&gt;Second, which is an important caveat, our model can’t pick up when teams play different players (e.g. backup goalies). In other words, if opponents started the year playing their backups in Chicago, but ended the year playing their starters at Chicago, we’d likely be calling what was really a change in opponent behavior a change in home advantage. Alternatively, if the Blackhawks started the year playing Corey Crawford in net for all of their home games but ended the year playing, for sake of argument, a beer-league goalie off of the street, our model would not pick up on that.&lt;/p&gt;
&lt;p&gt;Either way, hopefully this post highlights some of the potential that can be found when analyzing market numbers. We can be sure that the market values each teams’ home advantage different, and that by years’ end, Vegas was given the biggest adjustment when playing at home. It’s also possible that how the market views each teams home advantage is tied to how good that team is at a particular time point.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A state-space model to evaluate sports teams</title>
      <link>/post/a-state-space-model-to-evaluate-sports-teams/</link>
      <pubDate>Mon, 09 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/a-state-space-model-to-evaluate-sports-teams/</guid>
      <description>&lt;p&gt;A few years ago at a statistics conference, &lt;a href=&#34;https://www.luc.edu/math/ftfaculty/matthewsgregory.shtml&#34;&gt;Greg&lt;/a&gt;, &lt;a href=&#34;http://www.science.smith.edu/~bbaumer/w/&#34;&gt;Ben&lt;/a&gt; and I sat down to air a few greviences about our favorite sports. They were upset about baseball, and I was irked about hockey. Why the irritation?&lt;/p&gt;
&lt;p&gt;Relative to other popular sports like basketball and football, it seemed to us at the time that the best team was winning less often in baseball and hockey. And as fans wanting skilled teams to be rewarded, it was frustrating to so often have well-constructed teams fall short of titles.&lt;/p&gt;
&lt;p&gt;And so we decided to write a paper (pre-print &lt;a href=&#34;https://arxiv.org/abs/1701.05976&#34;&gt;here&lt;/a&gt;), now forthcoming in &lt;em&gt;Annals of Applied Statistics&lt;/em&gt;, that extends Glickman and Stern’s &lt;a href=&#34;https://homepages.cae.wisc.edu/~dwilson/rsfc/rate/papers/a-state-space-model.pdf&#34;&gt;state-space model&lt;/a&gt; to make comparisons between the NFL, NHL, NBA and MLB. In this post, I’ll describe the attraction of a state-space model, how to build one yourself (using the 2017-18 NHL season), and use our model estimates to answer a few hockey-specific questions. The entire analysis is reproducible, and you should be able to copy-and-paste your way into replicating the findings below.&lt;/p&gt;
&lt;div id=&#34;the-role-of-betting-market-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The role of betting market data&lt;/h2&gt;
&lt;p&gt;Team quality in sports is not static. Players get hurt, traded, or decide to retire. Managers are fired. Teams rest players, switch starting goalies, rotate a pitching staff, or decide they want to tank. All of these aspects make measuring team strength a bit dicey.&lt;/p&gt;
&lt;p&gt;Moreso than wins and losses or point differential, perhaps the best game-level metric of team strength comes not in what happens during the game, but what’s known before the game begins. Indeed, it’s been shown &lt;a href=&#34;https://www.jstor.org/stable/2287640&#34;&gt;time&lt;/a&gt; and &lt;a href=&#34;https://statsbylopez.files.wordpress.com/2013/08/jqas-2014-0058.pdf&#34;&gt;again&lt;/a&gt; that there’s no better way to judge models in sports than to compare to betting market data. Prior to each contest, betting markets put out probabilities associated with each team winning. The reason this data is so accurate is that it takes into account all of the factors above – injuries, opponent strength, line-ups, tanking, etc. And so as the saying goes – ``if you can’t beat em, use their data to develop a state-space model.’’ Or something like that.&lt;/p&gt;
&lt;p&gt;Assume home team &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; (listed at -130 on the money line) is playing away team &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; (+110). Punters backing &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; believe that squad has better than a &lt;span class=&#34;math inline&#34;&gt;\(\frac{130}{230} = 56.5\%\)&lt;/span&gt; chance of winning, while betters on team &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; assume it has at least a &lt;span class=&#34;math inline&#34;&gt;\(\frac{100}{210} = 47.6\%\)&lt;/span&gt; probability of winning. Accounting for the vig – which markets use to ensure a long-term profit – team &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; can be assumed to have a &lt;span class=&#34;math inline&#34;&gt;\(54.3\%\)&lt;/span&gt; chance of beating &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let’s take a look at what this data could look like. Here’s a sample of the 2017-18 NHL season (thanks to &lt;a href=&#34;https://twitter.com/pucktails&#34;&gt;Andy&lt;/a&gt; for sharing his spreadsheet, which loads using the &lt;code&gt;gsheet&lt;/code&gt; package).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse) 
library(rjags)
library(gsheet)
library(lubridate)
library(stringr)
library(knitr)

logit &amp;lt;- function(p) { 
  out &amp;lt;- log(p/(1 - p))
  return(out)
}

## Read in data from google sheets
urlA &amp;lt;- &amp;quot;https://docs.google.com/spreadsheets/d/1Hsmqphn9kGqWa88aYnimQxOvdV7ulv_jeIj0SYeBr3s/edit?usp=sharing&amp;quot;
nhl1718 &amp;lt;- gsheet2tbl(urlA)

## Improve the date format
nhl1718 &amp;lt;- nhl1718 %&amp;gt;% separate(date, c(&amp;quot;day&amp;quot;, &amp;quot;month&amp;quot;), &amp;quot;-&amp;quot;) %&amp;gt;% 
  mutate(date = ymd(paste(year, month, day)))

min.day &amp;lt;- min(nhl1718$date)
nhl1718 &amp;lt;- nhl1718 %&amp;gt;%
  mutate(day = date - min.day, week = as.numeric(floor(day/7) + 1))

tab.out &amp;lt;- head(nhl1718, 4) %&amp;gt;% select(date, home, away, p_home)
kable(tab.out)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;date&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;home&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;away&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p_home&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2017-10-04&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Winnipeg Jets&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Toronto Maple Leafs&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5284&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2017-10-04&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Pittsburgh Penguins&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;St. Louis Blues&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.6275&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2017-10-04&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Edmonton Oilers&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Calgary Flames&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5900&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2017-10-04&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;San Jose Sharks&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Philadelphia Flyers&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5810&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In the first four games of the year, Winnipeg, Pittsburgh, Edmonton, and San Jose were all favorites to some degree, with Pittsburgh the most plausible winner.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;building-a-state-space-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Building a state-space model&lt;/h2&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(y_{i,j,w} = \text{logit}(p_{i,j,w})\)&lt;/span&gt; (in our data, this is called &lt;code&gt;p_home&lt;/code&gt;) be the log-odds of the observed betting market probability in a game played between &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; during week &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt;. We take the &lt;code&gt;logit&lt;/code&gt; transform of each probability, which more closely aligns our outcomes with a normal distribution.&lt;/p&gt;
&lt;p&gt;We formally assume that:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{E}[\text{logit}(p_{i,j,w})] = \theta_{i,w} - \theta_{j,w} + \alpha,
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\theta_{i,w}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\theta_{j,w}\)&lt;/span&gt; reflect the corresponding team strengths of &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; during week &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; is the home advantage parameter. You’ll note that as &lt;span class=&#34;math inline&#34;&gt;\(\theta_{i,w}\)&lt;/span&gt; goes up, so to does team &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;’s probability of winning; followers of paired comparison comparison models will recognize this formulation as identical to Bradley-Terry.&lt;/p&gt;
&lt;p&gt;A state-space model assumes that the team strength estimates (&lt;span class=&#34;math inline&#34;&gt;\(\theta_{i,w}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\theta_{j,w}\)&lt;/span&gt;, etc.) evolve over time, with team &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;’s strength at week &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt; a function of both how good it was last week (&lt;span class=&#34;math inline&#34;&gt;\(\theta_{i,(w-1)}\)&lt;/span&gt;), as well as some prior noise. This is perfect for sports data – teams generally evolve slowly, and the best predictor of how good any given team is at a given time point is likely closely related to how good it was at the time just prior.&lt;/p&gt;
&lt;p&gt;Formally, we assume that for any given team &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\theta_{i,w} \sim N(\gamma\theta_{i,(w-1)},\tau_w^2),
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which imposes a first-order auto-regressive process to team strength (denoted by &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt;). Above, &lt;span class=&#34;math inline&#34;&gt;\(\tau_w^2\)&lt;/span&gt; corresponds to week-level uncertainty in the evolution of team strength. In the long run, we assume &lt;span class=&#34;math inline&#34;&gt;\(\gamma &amp;lt; 1\)&lt;/span&gt;, which prevents team strengths from exploding, and corresponds to the fact that, eventually, the best and worst teams in pro-sports will revert towards the league average.&lt;/p&gt;
&lt;p&gt;Here’s how we can built the model above, where we use the &lt;code&gt;rjags&lt;/code&gt; package in R to do some Bayesian inference using Gibbs sampling. First, we define our model inputs using outcome &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, week &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt;, and design matrix &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. For an &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;-game season played between &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;-number of teams, &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; corresponds to an &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; x &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; matrix, where each row contains exactly one 1 (for the home team) and one -1 (for the away team).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;y &amp;lt;- logit(nhl1718$p_home)
w &amp;lt;- nhl1718$week

#create a design matrix 
Teams &amp;lt;- sort(as.character(unique(c(as.character(nhl1718$home)))))

#Defining the number of things
nTeams &amp;lt;- length(Teams)
nWeeks &amp;lt;- max(nhl1718$week)
n &amp;lt;- nrow(nhl1718)

#Defining the design matrix
x &amp;lt;- matrix(0, nrow = dim(nhl1718)[1], ncol = length(Teams))
for (i in 1:dim(nhl1718)[1]) {
  x[i, which(as.character(nhl1718[i,&amp;quot;home&amp;quot;]) == Teams)] &amp;lt;- (1)
  x[i, which(as.character(nhl1718[i,&amp;quot;away&amp;quot;]) == Teams)] &amp;lt;- (-1)
} &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As an example, row 1, a game between home team Winnipeg (the 31st NHL team, alphabetically) and away team Toronto (27th), has a &lt;code&gt;1&lt;/code&gt; in the 31st column and a &lt;code&gt;-1&lt;/code&gt; in the 27th column, with the rest of the entries equal to zero.&lt;/p&gt;
&lt;p&gt;Next, our Bayesian approach requires prior distributions on the parameters of interest. I’m happy to answer specific questions if you’d like, but the BUGS code below corresponds to the models above, along with some additional variance parameters that use vague priors. The &lt;code&gt;rjags&lt;/code&gt; package is a bit outdated, but here’s &lt;a href=&#34;http://www.jkarreth.net/files/bayes-cph_Tutorial-JAGS.pdf&#34;&gt;one resource&lt;/a&gt; where you can learn more.&lt;/p&gt;
&lt;p&gt;Here’s what the BUGS code looks like:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model.string &amp;lt;-&amp;quot;
  model { 
for (i in 1:n) {
  y[i] ~ dnorm(mu[i], tauGame)
  mu[i] &amp;lt;- alpha + inprod(theta[w[i],],x[i,])
}
for (j in 1:nTeams){
  theta[1,j] ~ dnorm(0, tauSeason)
}
for (www in 2:nWeeks) {  
  for (j in 1:nTeams) {
    theta[www,j] ~ dnorm(gammaWeek*theta[www-1,j], tauWeek)
  }
}
alpha ~ dnorm(0,0.0001)
tauGame ~ dunif(0,1000) #uncertainty in outcome for each game
tauWeek ~ dunif(0,1000) 
tauSeason ~ dunif(0,1000) #variance parameter for the first week of the season
gammaWeek ~ dunif(0,1.5)
}
&amp;quot;
model.spec&amp;lt;-textConnection(model.string)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we’re ready to fit the model. In this example, I used 3 chains, 1000 posterior draws, and a thin of 5, which yields 3 sets of 200 draws for each parameter. The model converges quickly, otherwise we’d want to increase the number of draws at each step.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rjags)
n.chains &amp;lt;- 3 
n.adapt &amp;lt;- n.update &amp;lt;- n.draws &amp;lt;- 1000

posteriorDraws = c(&amp;#39;alpha&amp;#39;,&amp;#39;theta&amp;#39;)
thin &amp;lt;- 5
jags &amp;lt;- jags.model(model.spec,
                   data = list(&amp;#39;y&amp;#39; = y,&amp;#39;x&amp;#39; = x, &amp;#39;w&amp;#39; = w, &amp;#39;n&amp;#39; = n,&amp;#39;nTeams&amp;#39; = nTeams,&amp;#39;nWeeks&amp;#39; = nWeeks), 
                   n.chains = n.chains, n.adapt = n.adapt)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 1271
##    Unobserved stochastic nodes: 842
##    Total graph size: 47475
## 
## Initializing model&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;update(jags, n.update)
z &amp;lt;- jags.samples(jags, posteriorDraws, n.draws, thin = thin)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;examining-model-output&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Examining model output&lt;/h2&gt;
&lt;p&gt;Our output file &lt;code&gt;z&lt;/code&gt; contains information related to each of the parameters we were interested in. In this case, we have &lt;code&gt;z$alpha&lt;/code&gt; (home advantage) and &lt;code&gt;z$theta&lt;/code&gt; (team strengths), which have dimensions shown below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dim(z$alpha)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           iteration     chain 
##         1       200         3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dim(z$theta)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                     iteration     chain 
##        27        31       200         3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s generally a good idea to start a Bayesian analysis by assessing convergence – we do this extensively in our &lt;a href=&#34;https://arxiv.org/abs/1701.05976&#34;&gt;paper&lt;/a&gt; – but here’s one trace plot of &lt;code&gt;alpha&lt;/code&gt;, which shows that it’s generally well-behaved. Note the y-axis scale – given that our model uses log-odds as an outcome, each of our coefficients are likewise on a log-odds scale. Each color reflects one of the three chains from our Gibbs sampler.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;colours &amp;lt;- c(&amp;quot;#7fc97f&amp;quot;, &amp;quot;#beaed4&amp;quot;, &amp;quot;#fdc086&amp;quot;)
hfas &amp;lt;- data.frame(round(z$alpha[,,], 3))  %&amp;gt;% mutate(draw = 1:n())
hfas %&amp;gt;% ggplot(aes(draw, X1)) +
  geom_line(colour = colours[1]) + 
  geom_line(data = hfas, aes(draw, X2), colour = colours[2]) + 
  geom_line(data = hfas, aes(draw, X3), colour = colours[3]) + 
  xlab(&amp;quot;Draw&amp;quot;) + 
  ggtitle(&amp;quot;Home advantage (logit scale)&amp;quot;) + 
  ylab(&amp;quot;&amp;quot;) + 
  theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-09-a-state-space-model-to-evaluate-sports-team-performance_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The posterior distrbution of &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; is centered at about 0.208. This suggests that home teams, when playing a similarly talented opponent, have about a &lt;span class=&#34;math inline&#34;&gt;\(\frac{e^{0.208.}}{1 + e^{0.208}} = 55\%\)&lt;/span&gt; chance of winning. For those scoring at home, the NHL has the third best home advantage among the four sports we analyzed.&lt;/p&gt;
&lt;p&gt;Next, we examine team strengths.&lt;/p&gt;
&lt;p&gt;In the code below, I averaged each team strength over each week of the NHL season, and highlight the Vegas Golden Knights, a team that rose from the bottom of the league towards the top.&lt;/p&gt;
&lt;p&gt;As with the home advantage parameter, the y-axis reflects a log-odds scale. In this case, a coefficient of 0.5 reflects that a team has an &lt;span class=&#34;math inline&#34;&gt;\(\frac{e^{0.5}}{1+e^{0.5}} = 62\%\)&lt;/span&gt; chance of beating a league average team at a neutral site.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;avgs &amp;lt;- apply(z$theta, c(1,2), mean)
dims &amp;lt;- dim(avgs)
names(dims) &amp;lt;- c(&amp;quot;nweeks&amp;quot;, &amp;quot;nteams&amp;quot;)
df.beta &amp;lt;- data.frame(
    theta = as.vector(avgs),
    week = rep(1:dims[&amp;quot;nweeks&amp;quot;]), 
    team_id = rep(Teams, each =dims[&amp;quot;nweeks&amp;quot;]) )
vegas &amp;lt;- filter(df.beta, team_id == &amp;quot;Vegas Golden Knights&amp;quot;)

ggplot(df.beta, aes(week, theta, group = team_id)) + 
  geom_point(colour = &amp;quot;grey&amp;quot;) + 
  geom_line(colour = &amp;quot;grey&amp;quot;) + 
  geom_line(data = vegas, colour = &amp;quot;#b4975a&amp;quot;) + 
  geom_point(data = vegas, colour = &amp;quot;#b4975a&amp;quot;) + 
  ggtitle(&amp;quot;NHL team strengths by week of season, 2017-18&amp;quot;) + 
  ylab(&amp;quot;theta (log-odds scale)&amp;quot;) + 
  ylim(c(-0.56, 0.56)) + 
  annotate(&amp;quot;text&amp;quot;,x = 5, y = -0.4, label = &amp;quot;Vegas&amp;quot;, colour = &amp;quot;gold4&amp;quot;, size = 5) + 
  xlab(&amp;quot;Week&amp;quot;) + theme_bw(14)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-09-a-state-space-model-to-evaluate-sports-team-performance_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Vegas had a pretty incredible season.&lt;/p&gt;
&lt;p&gt;Of course, it’d be nice to see every team. Next, we use the &lt;code&gt;teamcolors&lt;/code&gt; package &lt;a href=&#34;https://github.com/beanumber/teamcolors&#34;&gt;(link)&lt;/a&gt; and a bit of &lt;code&gt;ggplot2&lt;/code&gt; wizardry to facet each NHL division in order to observe each team.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(teamcolors)
teamcolors1 &amp;lt;- teamcolors %&amp;gt;% filter(league == &amp;quot;nhl&amp;quot;) %&amp;gt;% rename(team_id = name) %&amp;gt;% 
  mutate(rand.color = ifelse(primary == &amp;quot;#010101&amp;quot;, secondary, primary))

df.beta1 &amp;lt;- df.beta %&amp;gt;% left_join(teamcolors1, by = c(&amp;quot;team_id&amp;quot; = &amp;quot;team_id&amp;quot;))

df.overall &amp;lt;- ggplot(data = df.beta1, 
       aes(x = week, y = theta, 
           color = team_id, fill = team_id)) +
  geom_line(alpha = 0.5) + 
  geom_point(shape = 21) + 
  ylim(c(-0.56, 0.56)) + 
  scale_color_manual(name = NULL, values = teamcolors1$primary) + 
  scale_fill_manual(name = NULL, values = teamcolors1$secondary) + 
  guides(color = FALSE, fill = FALSE) +
  theme_bw(base_size = 14)

df.overall + 
  geom_line(data = select(df.beta1, -division), color = &amp;quot;darkgray&amp;quot;, alpha = 0.3) + 
  geom_line(alpha = 0.5) + 
  geom_point(shape = 21, size = 0.5, alpha = 0.8) + 
  facet_wrap(~division, ncol = 2, dir = &amp;quot;v&amp;quot;) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + ylab(&amp;quot;&amp;quot;) + xlab(&amp;quot;Week&amp;quot;) + 
  ggtitle(&amp;quot;Team strength during the 2017/2018 NHL season&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-09-a-state-space-model-to-evaluate-sports-team-performance_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;My favorite part about this graph is the distinction between the two Divisions in the Eastern Conference. In the Atlantic Division, Boston, Tampa, and Toronto are three top teams, and apart from the Panthers, the other teams are bottom-dwellers. Meanwhile, nearly every Metropolitan team fits somewhere between the top and bottom of the Atlantic.&lt;/p&gt;
&lt;p&gt;Also impressive – check out Colorado’s slow improvement in the Central. The Avalanche started the year as the league’s third worst team (perception-wise), and are now in the playoffs.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;comparisons-among-teams&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Comparisons among teams&lt;/h2&gt;
&lt;p&gt;One benefit of using a Bayesian approach is that we can more precisely interpret team strength parameters. As an example, let’s look at the posterior distributions of team strength of three NHL teams, the Penguins, Flyers, and Hurricanes. For this task, I focus on the last six weeks of the NHL regular season, to roughly reflect team strength at season’s end.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;teams &amp;lt;- c(&amp;quot;Carolina Hurricanes&amp;quot;, &amp;quot;Pittsburgh Penguins&amp;quot;, &amp;quot;Philadelphia Flyers&amp;quot;)
thetas &amp;lt;- z$theta[20:26, Teams %in% teams, ,]
colors &amp;lt;- teamcolors1$secondary[Teams %in% teams]
team.1 &amp;lt;- data.frame(team_id = teams[1], beta = c(thetas[,1,,]))
team.2 &amp;lt;- data.frame(team_id = teams[2], beta = c(thetas[,2,,]))
team.3 &amp;lt;- data.frame(team_id = teams[3], beta = c(thetas[,3,,]))

df.matchup &amp;lt;- rbind(team.1, team.2, team.3) 
lmin &amp;lt;- quantile(df.matchup$beta, 0.01)
umin &amp;lt;- quantile(df.matchup$beta, 0.99)

df.matchup %&amp;gt;% ggplot(aes(beta, fill = team_id, group = team_id)) + 
  geom_density(alpha = 0.5) + 
  scale_fill_manual(name = NULL, values = colors) + 
  annotate(&amp;quot;text&amp;quot;, x = lmin, y = 4, label = &amp;quot;Hurricanes&amp;quot;,colour = colors[1], size = 5) + 
  annotate(&amp;quot;text&amp;quot;, x = .19, y = 4, label = &amp;quot;Flyers&amp;quot;, colour = colors[2], size = 5) + 
  annotate(&amp;quot;text&amp;quot;, x = umin, y = 4, label = &amp;quot;Penguins&amp;quot;, colour = colors[3], size = 5) +
  ggtitle(&amp;quot;Posterior draws of team strength&amp;quot;) + 
  xlab(&amp;quot;Team strength: log-odds scale&amp;quot;) + ylab(&amp;quot;Density&amp;quot;) + 
  guides(color = FALSE, fill = FALSE) + theme_bw(14) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-09-a-state-space-model-to-evaluate-sports-team-performance_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Above, the Penguins stand out as the best of the three teams. However, although it’s clear the Penguins are better than both the Flyers and the Hurricanes – note the lack of overlap in the density plots – it’s not as clear that the Flyers are better than the Hurricanes.&lt;/p&gt;
&lt;p&gt;Using samples from the posterior distribution, we can be a bit more precise: there’s only about a 1 in 2000 chance that the Hurricanes are better than the Penguins, a 1 in 150 chance that the Flyers are better than the Penguins, and a 1 in 5 chance that the Hurricanes are better than the Flyers.&lt;/p&gt;
&lt;p&gt;But here’s where the NHL is tricky (e.g, random).&lt;/p&gt;
&lt;p&gt;Knowing that the Penguins are better than the Flyers with probability 99.4 percent does not precisely correspond to their chances of actually beating the Flyers when the two teams square off in the 1st round of the 2018 NHL playoffs. In fact, we can use our team strength estimates to sample what a 7-game playoff series could look like, and check how often each team wins. Below, I assume that the Penguins have the home advantage.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(0)
flyers.strength &amp;lt;- sample(team.2$beta, 7)
penguins.strength &amp;lt;- sample(team.3$beta, 7)
home.advantage &amp;lt;- sample(z$alpha, 7)
logit.games &amp;lt;- penguins.strength - flyers.strength + 
  home.advantage*c(1, 1, -1, -1, 1, -1, 1)
prob.games &amp;lt;- exp(logit.games)/(1 + exp(logit.games))
penguin.wins &amp;lt;- rbinom(7, 1, prob = prob.games)
do.penguins.win &amp;lt;- sum(penguin.wins) &amp;gt;= 4
do.penguins.win&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the example above, the Penguins emerge victorious – congrats &lt;a href=&#34;https://www.nhl.com/penguins/news/penguins-hire-sam-ventura-as-director-of-hockey-research/c-289961770&#34;&gt;Sam&lt;/a&gt;!! – but that doesn’t happen every iteration. Here’s what 10,000 series outcomes would look like:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(0)
penguins.sim.wins &amp;lt;- 0
for (i in 1:10000){
  flyers.strength &amp;lt;- sample(team.2$beta, 7)
  penguins.strength &amp;lt;- sample(team.3$beta, 7)
  home.advantage &amp;lt;- sample(z$alpha, 7)
  logit.games &amp;lt;- penguins.strength - flyers.strength + home.advantage*c(1, 1, -1, -1, 1, -1, 1)
  prob.games &amp;lt;- exp(logit.games)/(1 + exp(logit.games))
  penguin.wins &amp;lt;- rbinom(7, 1, prob = prob.games)
  do.penguins.win &amp;lt;- sum(penguin.wins) &amp;gt;= 4
  penguins.sim.wins &amp;lt;- penguins.sim.wins + do.penguins.win
}
sum(penguins.sim.wins)/10000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6634&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our numbers suggest that the Penguins would beat the Flyers roughly 2 out of every 3 times they played a 7-game series. Unsurprisingly, this almost exactly matches the probability implied in sportsbook series odds, which currently price the Penguins at -225 and the Flyers at +195.&lt;/p&gt;
&lt;p&gt;One last thought: how much is the home advantage worth for Pittsburgh? Tweaking the code above, the Flyers would win about 4% more often if they were the team with an extra home game. In other words, only roughly 1 in 25 series outcomes would change were the Penguins to lose their home advantage to the Flyers. This does not appear to be the answer that my Twitter followers &lt;a href=&#34;https://twitter.com/StatsbyLopez/status/983686397818298368&#34;&gt;expected&lt;/a&gt;, as 1 in 25 was the least likely of four outcomes (10, 15, 20, 25).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;The above code implements a Bayesian state-space model to etimate team strength in the NHL season using betting market numbers. Though the &lt;code&gt;rjags&lt;/code&gt; package is not the most user-friendly for first-time practitioners, it provides a mechanism for fitting complex models using R. The resulting team strengths are fun to graph and expore, and I encourage you to try it out yourself.&lt;/p&gt;
&lt;p&gt;In future work, there are a few questions that I hope to tackle: Can our model detect if certain teams have better home advantages than others? Was there a so-called &lt;code&gt;Vegas flu&lt;/code&gt;? And when did betting markets begin to pick up on it? What is the probability that any given team is better than any given other team? How did the NHLs shifting postseason structure (to a divisional format) impact postseason chances?&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>On log-loss and scoring the NCAA tournament</title>
      <link>/post/on-log-loss-in-the-ncaa-tournament/</link>
      <pubDate>Mon, 26 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/on-log-loss-in-the-ncaa-tournament/</guid>
      <description>&lt;p&gt;In 2014, &lt;a href=&#34;www.statsinthewild.com&#34;&gt;Greg&lt;/a&gt; and I won the first annual Kaggle March Madness contest. This led to really cool things happening to a pair of nondescript statisticians that, to the likely detriment of our social lives, happened to know where to find good basketball data while also remembering, on a tight deadline, to enter &lt;code&gt;type = &amp;quot;response&amp;quot;&lt;/code&gt; in our &lt;code&gt;glm&lt;/code&gt; code.&lt;/p&gt;
&lt;p&gt;The Kaggle victory was simultaneously awesome – like &lt;a href=&#34;https://www.nytimes.com/2015/03/22/opinion/sunday/making-march-madness-easy.html&#34;&gt;New York Times&lt;/a&gt; awesome – and embarassing, as Greg and I realized how lucky we were to have finished on top.&lt;/p&gt;
&lt;p&gt;To wit:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Even if we assumed that our model contained the true probabilities for every 2014 NCAA tournament game, &lt;a href=&#34;https://statsbylopez.files.wordpress.com/2013/08/jqas-2014-0058.pdf&#34;&gt;there was a better chance that our submission finished outside the top-10&lt;/a&gt; than inside it. It’s humbling to know that you can have precise numbers for every possible game, and in all likelhood, not have increased your chances of winning by much more than a factor of five.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We are pretty sure we actually mis-typed Ohio State as Ohio during our data wrangling process, thus giving what was a pretty good Ohio State team a fairly mediocre ranking. This helped us immensely when Ohio State lost to Dayton in a first round upset. Note the caveat that I am “pretty sure” this was the case; honestly, that’s about as good as I can get looking at our old files.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We won on a disqualification, when someone who had submitted multiple entries was banned from winning.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It’s a few years later now, but the luck required for us to have won the Kaggle contest has stayed with us. And it’s an important backdrop for the point of this post – &lt;strong&gt;it’s time for Kaggle to change its scoring rule&lt;/strong&gt;.&lt;/p&gt;
&lt;div id=&#34;log-loss-as-a-scoring-rule.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Log loss as a scoring rule.&lt;/h2&gt;
&lt;p&gt;Kaggle scores contest submissions using &lt;a href=&#34;https://www.r-bloggers.com/making-sense-of-logarithmic-loss/&#34;&gt;log-loss (LL)&lt;/a&gt;, and allows participants two unique submissions. As a scoring rule, LL boasts a few critical properties:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;LL is a &lt;a href=&#34;https://www.stat.washington.edu/raftery/Research/PDF/Gneiting2007jasa.pdf&#34;&gt;proper scoring rule&lt;/a&gt; – that is, in expectation, the best log-loss is obtained by reporting the true probabilities.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Penalties are disproportionately larger for high-confident decisions that go wrong. As an example, giving Virginia a 97% probability against UMBC results in a LL of 3.5 (higher is worse), while a coin flip of 0.5 scores only a LL of 0.7. But note how much better 95% would have been – (LL of 3) – relative to a 99% submission (LL of 4.6).&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In the long run of hypothetical NCAA tournament games between a fixed number of Kaggle participants, LL makes sense as a scoring system. But the NCAA tournament scores only 63 games, and the number of Kaggle participants has nearly quadrupled in the last four years. That’s a tricky setting moving forward.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;limitations-of-log-loss&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Limitations of Log loss&lt;/h2&gt;
&lt;p&gt;There are a few important limitations to log loss as a scoring rule with NCAA tournament data (see Christopher Long’s post &lt;a href=&#34;angrystatistician.blogspot.com/2017/04/why-does-kaggle-use-log-loss.html&#34;&gt;here&lt;/a&gt; for more).&lt;/p&gt;
&lt;p&gt;First, the small sample of games means that submissions maximizing your chance of finishing first are &lt;em&gt;not&lt;/em&gt; the same as those minimizing your expected log-loss. The 2017 winner, Andrew Landgraf, has a &lt;a href=&#34;http://blog.kaggle.com/2017/05/19/march-machine-learning-mania-1st-place-winners-interview-andrew-landgraf/&#34;&gt;fascinating interview&lt;/a&gt; in which he identifies that his strategy was to &lt;em&gt;not&lt;/em&gt; choose his most accurate probabilities. Instead, Andrew created hypothetical distributions of what he thought other participants would submit, and used these simulations to derive an optimal set of predictions.&lt;/p&gt;
&lt;p&gt;Second, and along similar lines, there is now a massive, and growing, incentive to predict upsets. With the 2014 data, I estimated that, under a few assumptions, picking a single first round upset with probability 1 would have increased our chances of winning from about 1 in 40 to about 1 in 15. That’s a massive gain, and its a strategy that several recent teams inside the top-5 have employed. As one example, check out the team BAYZ – in 2016, BAYZ finished &lt;a href=&#34;https://www.kaggle.com/c/march-machine-learning-mania-2016/leaderboard&#34;&gt;third overall&lt;/a&gt;. &lt;a href=&#34;https://www.kaggle.com/c/march-machine-learning-mania-2017/leaderboard&#34;&gt;Last year&lt;/a&gt; and &lt;a href=&#34;https://www.kaggle.com/c/mens-machine-learning-competition-2018/leaderboard&#34;&gt;this year&lt;/a&gt;, however, BAYZ will finish in the bottom 15. Did BAYZ get worse? No. But it is picking upsets, and both living and dying with the results. And the more teams that pick upsets, the more of an uphill fight that those who aren’t picking upsets will have. Note that even though we know it makes our chances of winning worse, Greg and I have opted against picking upsets.&lt;/p&gt;
&lt;p&gt;Third, combining LL with the prize structure of Kaggle – only the top few finishers pay out, and the prizes are massive – encourages cheating, where people submit multiple entries under different names. As mentioned above, Greg and I only won when someone else got disqualified for multiple entries. Why did this person enter multiple times? He wanted to game LL by choosing upsets. How did this person enter multiple times? He entered the same submission, just changing one unique upset on each sheet. And why did he finish in front of us? Because Mercer beat Duke in 2014, and so instead of a typical LL penalty of about 3 (&lt;code&gt;log(0.05) = -3&lt;/code&gt;), his penalty for the Duke/Mercer game was 0. Fortunately, the good folks at Kaggle were able to catch that cheater and drop him from the leaderboard … but I’m not so sure that will always be the case moving forward.&lt;/p&gt;
&lt;p&gt;Fourth, and finally, in allowing &lt;em&gt;two&lt;/em&gt; entries, the current Kaggle rules are essentially doubling the options that entrants have to pick upsets. Like several other participants, Greg and I enter the same model, changing only the final game: on one sheet, we enter 1’s for the teams on the left side of the bracket, and on the other sheet, we enter 1’s for teams on the right side of the bracket. This ad-hoc scoring is not without motivation – using simulations in the 2014 tournament, I estimated that our chances of winning roughly doubled when employing this strategy – but the overall strategy as a whole does seem disingenous.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-other-scoring-rules-are-possible&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What other scoring rules are possible?&lt;/h2&gt;
&lt;p&gt;In an ideal world, Kaggle would reward participants for accuracy and not luck. The current set-up, however, disproportionately rewards upsets and contrarion strategies. In place, here are a few options to consider.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Predict the point differential, and score using RMSE or MAE.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Point differential is commonly mentioned in the Kaggle forums as an alternative outcome, as it boasts a few benefits over LL. First, upset-based strategies are not as obvious. If Virginia is a 20-point favorite over UMBC, a rogue Kaggler predicting a 41-point Virginia win would be taking the same risk as one predicting a one-point UMBC win. Second, point-differential tells us more about the participating teams. This is why statistical models using point differential as an outcome almost always perform better than ones that use win-loss results as an outcome.&lt;/p&gt;
&lt;p&gt;Note that a common response to this suggestion is “I don’t want results being swayed by bench players in garbage time.” However, these swings are minor in comparison to the enormous swings that are already currently occuring with won-loss outcomes. As an example, a Kaggler with Loyola 0.51 &amp;gt; Miami assuredly had the better prediction that one with Loyola 0.99 &amp;gt; Miami (Loyola won on a last second shot). However, the second Kaggler receives a better score than the first. This does not seem fair.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Predict the point differential, score using RMSE or MAE, and merge with current LL predictions.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Having two scoring rules would make the contest a bit more challenging to enter, but the benefit of having multiple approaches for measuring model performance could be worth it. In this instance, whoever finishes with the best average rank under each scoring rule would be the winner.&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Predict point differential and uncertainty in that point differential, and score using the normal distribution.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I think this is my preferred scoring rule, and it is inspired by Micah’s &lt;a href=&#34;http://hockeyviz.com/txt/predictionContest1718&#34;&gt;Sour Candy Contest&lt;/a&gt; in the NHL.&lt;/p&gt;
&lt;p&gt;In this set-up, participants submit a prediction for point differential as well as a standard deviation to reflect uncertainty in that point differential. The score associated with each game combines the predicted result, the observed result, and the standard deviation, by estimating the likelihood that the observed result occurs given the . Though it could be rescaled, the maximum score for any given game is 1 – this occurs when a prediction is perfectly accurate and the proposed standard deviation is 0.&lt;/p&gt;
&lt;p&gt;As a hypothetical, consider Loyola versus Miami, which finished as a two-point Loyola win.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Submission 1: &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mu}_1\)&lt;/span&gt; = 2, &lt;span class=&#34;math inline&#34;&gt;\(\hat{\sigma}\)&lt;/span&gt; = 1.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The impact of make-up calls is probably bigger than you think</title>
      <link>/post/the-impact-of-make-up-calls-is-probably-bigger-than-you-think/</link>
      <pubDate>Fri, 16 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/the-impact-of-make-up-calls-is-probably-bigger-than-you-think/</guid>
      <description>

&lt;p&gt;One of the more reliable indicators of which NHL team is likely to get the next power play has little to do with score or style of play. Instead, it&amp;rsquo;s been shown repeatedly (&lt;a href=&#34;https://fivethirtyeight.com/features/hockey-refs-are-out-to-get-you-if-they-already-got-the-other-guy/&#34; target=&#34;_blank&#34;&gt;Ex 1&lt;/a&gt;, &lt;a href=&#34;http://people.stat.sfu.ca/~tim/papers/penalty.pdf&#34; target=&#34;_blank&#34;&gt;Ex 2&lt;/a&gt;) that referees call a substantially higher number of make-up penalties than would otherwise be expected in order to maintain an overall even number of violations on each team. Call it a &lt;a href=&#34;https://creativematter.skidmore.edu/cgi/viewcontent.cgi?article=1004&amp;amp;context=math_fac_schol&#34; target=&#34;_blank&#34;&gt;&lt;em&gt;biased impartiality&lt;/em&gt;&lt;/a&gt; &amp;ndash; in order to appear impartial by game&amp;rsquo;s end, ref&amp;rsquo;s let previous decisions drive future ones.&lt;/p&gt;

&lt;p&gt;A common line of reasoning behind call reversals is that referees would prefer to $not$ be part of the final narrative as to why a particular team won or lost. If each team has a relatively similar number of power plays, media and fans clamoring that one team was favored would have less support. Turns out, however, that by evening up penalty calls, hockey ref&amp;rsquo;s are impacting the same game narratives that they are trying to avoid being a part of. In this post, I&amp;rsquo;ll both confirm the existance of make-up calls and extend a similar analysis to look at the impact of make-up calls on game outcomes.&lt;/p&gt;

&lt;h2 id=&#34;the-make-up-call&#34;&gt;The make-up call&lt;/h2&gt;

&lt;p&gt;Claiming that a call in any sport is due to anything besides an unbiased referee assessment is a non-trivial task. Fortunately, hockey boasts several settings that make looking for make-up calls somewhat feasible. Game&amp;rsquo;s are often tied, and looking at penalties in tied-game states can avoid results being impacted by changes in style of play due to the score. Moreover, with an average of about eight minors a game &amp;ndash; most of them judgement calls &amp;ndash;  there&amp;rsquo;s a large enough sample size within each game on which to evaluate ref choices.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s an initial chart comparing the penalty differential between each team at the end of the first period (y-axis, where positive means more penalties) to penalty differential over the remainder of the game (shown in the darkness and color of the grid).&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; The x-axis corresponds to the home team&amp;rsquo;s estimated pre-game win probability, calculated using odds implied by betting markets (you can get that data &lt;a href=&#34;https://github.com/bigfour/competitiveness/blob/master/data/bigfour_public.rda&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;). Accounting for relative team talent is potentially important, as its a variable that&amp;rsquo;s quite possibly linked to penalty calls and team aggressiveness. Only games that were tied at the end of the first period are included in this chart &amp;ndash; the data goes from the 05-06 to 15-16 regular seasons.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/makeupF1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;When home teams have two more infractions than their opponent at the end of the first period, they average roughly 0.8 fewer infractions the rest of the game. Alternatively, when home teams have two fewer penalties at the end of the first, they average about 0.5 more violations in periods 2 and 3. The difference between having more and fewer penalties in tied games is quite significant from a statistical perspective. Of course, if you had read a few of the links above, you probably would have guessed that there would be this type of impact. But the effect size (an extra 1.3 power plays in 2 periods), as well as the shading consistency of the chart above, seemed noteworthy.&lt;/p&gt;

&lt;h2 id=&#34;the-impact-of-the-make-up-call&#34;&gt;The impact of the make-up call&lt;/h2&gt;

&lt;p&gt;A make-up calls is all well and good until one team loses a game because of it.&lt;/p&gt;

&lt;p&gt;To check how future games are impacted by make-up calls, I used a chart similar to the one shown above. In this one, however, shading reflects the percent of games eventually won by the home team.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/makeupF2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In this version, the shading is reversed &amp;ndash; more first period penalties are linked to an increased win rate, just as fewer penalties are linked to a decreased win rate. Again, remember that we&amp;rsquo;re only looking at tied games, and we&amp;rsquo;re accounting for how good each team is!&lt;/p&gt;

&lt;p&gt;Altogether, when the game is expected to be a relative toss-up, the difference between having two more first period penalties and two fewer first period penalties is about 7 or 8 percentage points in terms of the home team&amp;rsquo;s chances. The effect is a bit larger when the home team is not expected to win, with differences between fewer and more first period worth an estimated 10 percentage points.&lt;/p&gt;

&lt;p&gt;While this may seem like a relatively minor change, the fact that there&amp;rsquo;s $any$ change in win rate based on prior penalties is curious given that we&amp;rsquo;re only looking at tied games. Additionally, we may be underestimating the impact of make-up calls &amp;ndash; if a team had more first period penalties, it would have likewise been more likely to start the second period on a penalty kill, a quirk that I did not account for above. Further, this isn&amp;rsquo;t a peculiar set of games  &amp;ndash; about a third of NHL games end the first period tied. Finally, although it&amp;rsquo;d require further assumptions with respect to playing style, it&amp;rsquo;s also likely that make-up calls have an impact on games that aren&amp;rsquo;t tied.&lt;/p&gt;

&lt;p&gt;One limitation of this post is that, in using observational data, it is feasible that there&amp;rsquo;s something else responsible for the link between first period penalty differential and game outcomes. One possible alternative is that teams that have taken penalties suddenly feel the need to be more aggressive, which then leads to more natural reversals masquerading as make-up calls. However, for tied games, I can&amp;rsquo;t imagine that the revenge factor looms too large. Further, in &lt;a href=&#34;https://creativematter.skidmore.edu/cgi/viewcontent.cgi?article=1004&amp;amp;context=math_fac_schol&#34; target=&#34;_blank&#34;&gt;previous work&lt;/a&gt;, I found that penalty reversals were, if anything, higher in the postseason, where there is almost no desire for revenge (at least via penalties).&lt;/p&gt;

&lt;p&gt;If you are curious about the coding for the project, I used play-by-play data via the now archived &lt;a href=&#34;https://cran.r-project.org/web/packages/nhlscrapr/index.html&#34; target=&#34;_blank&#34;&gt;nhlscrapr&lt;/a&gt; package in R. Matching minors, as well as all major penalties, were dropped. The code for all figures, models, and numbers is &lt;a href=&#34;https://github.com/statsbylopez/BlogPosts/blob/master/NHL_predict_penaltydiff.R&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;I used a generalized additive model (GAM) of penalty differential as a factor of both home team win probability and first period penalty differential. In this and the example below, first period penalty differential was a significant predictor. GAMs make sense in this and other examples, as the true model fit with penalty outcomes is unknown.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;Same model as above, now with a binary outcome.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>On the risks of categorizing a continuous variable (with an application to baseball data)</title>
      <link>/post/on-the-risks-of-categorizing-a-continuous-variable/</link>
      <pubDate>Wed, 07 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/on-the-risks-of-categorizing-a-continuous-variable/</guid>
      <description>&lt;div id=&#34;to-err-is-to-human&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;To err is to human&lt;/h3&gt;
&lt;p&gt;In the third inning during a contest a few weeks back between the Nationals and Cubs, Washington’s Brian Goodwin hit a line drive to left field with two outs and a runner on third. Despite an initial pause, Chicago’s Kyle Schwarber ran in and attempted to field the ball around his knees.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/Kyle.gif&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Ruled an error on Schwarber, the play gave the a Nationals run in an eventual 9-4 win. This struck me as an odd ruling. Schwarber could have easily stood still, retrieved the ball on a hop, and not been given an error. Instead, by trying to field the hard hit ball and missing, he was punished. Note that Goodwin did not reach second after Schwarber’s mishap.&lt;/p&gt;
&lt;p&gt;Intracacies of the baseball rulebook are out of my realm of expertise, but that specific play got me thinking about what goes into decisions to reward an error. Primarily – is the difficulty of the play (including the exit velocity of the hit) taken into account? Fielding a sharp ground ball or line drive, no matter where its hit, would seem excessively more difficult than fielding a dribbler or pop fly. Alternatively, one could argue that its the slowest ground balls that are the most problematic, given that a fielder may need to rush his throw.&lt;/p&gt;
&lt;p&gt;The point of this post will be to identify a few predictors of error rates, starting with a sidebar about categorizing continuous variables.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exit-velocity-is-so-good-..-it-predicts-higher-and-lower-error-rates.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Exit velocity is so good .. it predicts higher &lt;em&gt;and&lt;/em&gt; lower error rates.&lt;/h3&gt;
&lt;p&gt;Using the last two seasons of Statcast data, and with a huge hat tip to Bill Petti’s &lt;a href=&#34;http://billpetti.github.io/baseballr/&#34;&gt;baseballr&lt;/a&gt; package, I grabbed all potential putouts (any play ruled an out, error, sac fly, bunt, etc). The Statcast data is super useful, as for games since 2015, it contains exit velocity and launch angle for each ball in play.&lt;/p&gt;
&lt;p&gt;My first interest lies in whether or not exit velocity is linked to error chances. One popular strategy (both in and outside of sports analytics) is to categorize data. In other words, group balls in play together, bin hits with similar velocities, and check for the frequency of errors within each bin. That was my first step with the Statcast data. But because I was unsure what categories to use, I tried two different sets: (i) 70 mph or less, 70-85, 85 or more and (ii) 80 or less, 80-95, 95 or more. Each was roughly meant to align with softly hit, medium hit, and sharply hit balls, and each contained several thousand balls in play.&lt;/p&gt;
&lt;p&gt;Here’s a chart showing the fraction of plays resulting in an error in each bin. The point in each graph is the average number of errors on all balls in play for each bin.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-03-07-on-the-risks-of-categorizing-a-continuous-variable_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Your eyes are not deceiving you.&lt;/p&gt;
&lt;p&gt;On the left graph, there’s a drop in error rate with increased exit velocity – on the right, there’s an increase in the rate of errors. That is, with identical data and only a slight shift in groupings, you could make either claim and feel justified. Errors appear to both increase and decrease with harder hit balls, and both contrasts are statistically significant (those are 95% intervals included with each point).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;so-which-is-it&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;So which is it?&lt;/h3&gt;
&lt;p&gt;In fairness to the above analysis, both charts are correct. In unfairness to the above, such analysis is often problematic, and its no accident that the findings conflict.&lt;/p&gt;
&lt;p&gt;Specifically, categorizing a continuous variable and looking within each category or bin can lead to a host of statistical problems. For an excellent overview, see a Vanderbilt biostat write-up &lt;a href=&#34;http://biostat.mc.vanderbilt.edu/wiki/Main/CatContinuous&#34;&gt;here&lt;/a&gt;, or read a more thorough paper by Caroline Bennette and Andrew Vickers &lt;a href=&#34;https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-12-21&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Two problems discussed above specifically come into play with respect to our binning above.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Categorization assumes that there is a discontinuity in response (error rates) as interval boundaries are crossed.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Categorization assumes that the relationship between the predictor (exit velocity) and the response (error rates) is flat within intervals.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Indeed, error rates have a slightly more complicated relationship with exit velocity than can be explained by intervals, and when diving deeper, it’s evident that there’s no natural discontinuity in the rates of errors that we could even try to use.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;an-alternative-strategy-for-measuring-error-rates.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;An alternative strategy for measuring error rates.&lt;/h3&gt;
&lt;p&gt;Instead of binning, a more appropriate technique would involve modeling the rates of errors given exit velocity.&lt;/p&gt;
&lt;p&gt;Of course, in light of the above charts, the link between error rates and exit velocity may be complex, and traditional logistic regression may not be sufficient. Instead, I used a generalized additive model (GAM) that included smoothed terms for both exit velocity and launch angle. GAMs can be great with sports data – they are easy to implement, explicitly include cross-validation as part of the fitting process, and don’t require an exact model specification. Given that we don’t know the best way to specify a model of errors given launch speed, GAMs are a natural fit.&lt;/p&gt;
&lt;p&gt;One final plus? GAMs also make for nice visualizations, something that every sports analyst should always be working on. If you want to read more about GAMs, start with the Stitch Fix &lt;a href=&#34;http://multithreaded.stitchfix.com/blog/2015/07/30/gam/&#34;&gt;tutorial&lt;/a&gt;, or I’ll shamelessly promote my work on NFL referees (&lt;a href=&#34;https://statsbylopez.files.wordpress.com/2013/08/lopez-2016-economic_inquiry.pdf&#34;&gt;link&lt;/a&gt;) and Brian’s work on MLB umpires (&lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1002/mde.2630/abstract&#34;&gt;link&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Here’s a chart showing the estimated error rate given exit velocity (x axis) and launch angle (y axis). Darker shades are linked to higher error rates.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: nlme&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;nlme&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:dplyr&amp;#39;:
## 
##     collapse&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## This is mgcv 1.8-22. For overview type &amp;#39;help(&amp;quot;mgcv-package&amp;quot;)&amp;#39;.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-03-07-on-the-risks-of-categorizing-a-continuous-variable_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Turns out, there’s an obvious driver of the error rate weirdness – sacrifices!&lt;/p&gt;
&lt;p&gt;A few thousand of our plays feature balls hit at roughly 40 mph going (almost) directly into the ground. While not all of these plays are attempted bunts, many appear to be (according to the Statcast play by play descriptions). Turns out, bunts are linked to high error rates (as high as 14%), likely on account of how quickly the pitcher and infielders need to react.&lt;/p&gt;
&lt;p&gt;Returning to our original question, there does appear to be a slight increase in error rates with increased exit velocity after discarding bunt attempts. Although balls hit between 50 and 90 mph all yield roughly have the same chances of an error (somewhere around 2%) – error chances increase with the hardest hit balls. With exit velocities of 110 mph or higher, for example, our model expects error rates at several launch angles to be more than 5%.&lt;/p&gt;
&lt;p&gt;Note that one downside of the chart is that we don’t get to see how many balls are hit at each launch speed and angle. For example, the dark right corner of the chart only includes a few fieldable plays, and these estimates likely come with substantial error. However, we can crosscheck some of the dark spots using the actual data. Sure enough, balls hit with an exit velocity of 100 mph or higher within +/5 degrees of 0 yield an error rate of 4.9% (there were about 5400 such instances). Hit it 105 mph or harder in that same zone, and the error rate jumps to 6%.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;whats-the-take-home&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;What’s the take home?&lt;/h3&gt;
&lt;p&gt;From a statistical perspective, hopefully you can recognize both the dangers of categorizing continuous data, as well as the attractive features offered by a GAM (and if you want to try a GAM yourself – the code is up &lt;a href=&#34;https://github.com/statsbylopez/statsbylopez.github.io&#34;&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;From a baseball perspective, the hardest hit balls do increase error rates. Additionally, with my intuition being that errors are often discarded from the perspective of analyzing hitter talent, this type of error creation could be worth thinking more about. In addition to a side benefit of someone who hits the ball as hard as Aaron Judge or Giancarlo Stanton, having good, capable bunters &lt;em&gt;could&lt;/em&gt; actually be undervalued. Putting down a sacrifice is generally not considered worth it (trading an out for moving up a base), but with error rates as high as 14%, there may be more to the story.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
